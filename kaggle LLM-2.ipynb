{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!cp /kaggle/input/datasets-wheel/datasets-2.14.4-py3-none-any.whl /kaggle/working\n!pip install  /kaggle/working/datasets-2.14.4-py3-none-any.whl\n!cp /kaggle/input/backup-806/util_openbook.py .","metadata":{"execution":{"iopub.status.busy":"2023-10-07T12:35:43.634988Z","iopub.execute_input":"2023-10-07T12:35:43.635323Z","iopub.status.idle":"2023-10-07T12:36:21.126981Z","shell.execute_reply.started":"2023-10-07T12:35:43.635296Z","shell.execute_reply":"2023-10-07T12:36:21.125524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# installing offline dependencies\n!pip install -U /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n!cp -rf /kaggle/input/sentence-transformers-222/sentence-transformers /kaggle/working/sentence-transformers\n!pip install -U /kaggle/working/sentence-transformers\n!pip install -U /kaggle/input/blingfire-018/blingfire-0.1.8-py3-none-any.whl\n\n!pip install --no-index --no-deps /kaggle/input/llm-whls/transformers-4.31.0-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/llm-whls/peft-0.4.0-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/llm-whls/trl-0.5.0-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2023-10-07T12:36:21.129758Z","iopub.execute_input":"2023-10-07T12:36:21.130363Z","iopub.status.idle":"2023-10-07T12:38:21.052694Z","shell.execute_reply.started":"2023-10-07T12:36:21.130327Z","shell.execute_reply":"2023-10-07T12:38:21.051517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nfrom datasets import load_dataset, load_from_disk\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport torch\nfrom transformers import LongformerTokenizer, LongformerForMultipleChoice\nimport transformers\nimport pandas as pd\nimport pickle\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport unicodedata\nfrom transformers import AutoTokenizer, AutoModelForMultipleChoice\nimport os\nfrom dataclasses import dataclass\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\nfrom typing import Optional, Union\nfrom datasets import Dataset\nfrom torch.utils.data import DataLoader\nimport gc","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2023-10-07T12:38:21.054642Z","iopub.execute_input":"2023-10-07T12:38:21.055012Z","iopub.status.idle":"2023-10-07T12:38:32.498876Z","shell.execute_reply.started":"2023-10-07T12:38:21.054979Z","shell.execute_reply":"2023-10-07T12:38:32.497793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp -r /kaggle/input/stem-wiki-cohere-no-emb /kaggle/working\n!cp -r /kaggle/input/all-paraphs-parsed-expanded /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2023-10-07T12:38:32.501232Z","iopub.execute_input":"2023-10-07T12:38:32.501811Z","iopub.status.idle":"2023-10-07T12:38:55.736608Z","shell.execute_reply.started":"2023-10-07T12:38:32.501776Z","shell.execute_reply":"2023-10-07T12:38:55.735199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 定义一个函数，将列表分割成指定大小的子列表\ndef SplitList(mylist, chunk_size):\n    return [mylist[offs:offs+chunk_size] for offs in range(0, len(mylist), chunk_size)]\n\n# 定义一个函数，从验证数据集中检索相关的文档\ndef get_relevant_documents_parsed(df_valid):\n    df_chunk_size=600  # 设置每个数据块的大小\n    # 从指定路径加载预处理过的数据集\n    paraphs_parsed_dataset = load_from_disk(\"/kaggle/working/all-paraphs-parsed-expanded\")\n    # 修改文本，将标题、段落和文本合并，并替换换行符和单引号\n    modified_texts = paraphs_parsed_dataset.map(lambda example:\n                                             {'temp_text':\n                                              f\"{example['title']} {example['section']} {example['text']}\".replace('\\n',\" \").replace(\"'\",\"\")},\n                                             num_proc=2)[\"temp_text\"]\n\n    # 初始化两个列表，用于存储文章的索引和值\n    all_articles_indices = []\n    all_articles_values = []\n    # 使用 tqdm 进度条，遍历验证数据集，每次处理一个数据块\n    for idx in tqdm(range(0, df_valid.shape[0], df_chunk_size)):\n        df_valid_ = df_valid.iloc[idx: idx+df_chunk_size]  # 获取当前数据块\n    \n        # 调用 retrieval 函数，获取相关文章的索引和得分\n        articles_indices, merged_top_scores = retrieval(df_valid_, modified_texts)\n        # 将结果添加到列表中\n        all_articles_indices.append(articles_indices)\n        all_articles_values.append(merged_top_scores)\n        \n    # 将所有结果连接成数组\n    article_indices_array = np.concatenate(all_articles_indices, axis=0)\n    articles_values_array = np.concatenate(all_articles_values, axis=0).reshape(-1)\n    \n    # 获取每个查询的文章数量\n    top_per_query = article_indices_array.shape[1]\n    # 展平文章信息，并按查询分组\n    articles_flatten = [(\n                         articles_values_array[index],\n                         paraphs_parsed_dataset[idx.item()][\"title\"],\n                         paraphs_parsed_dataset[idx.item()][\"text\"],\n                        )\n                        for index,idx in enumerate(article_indices_array.reshape(-1))]\n    # 使用 SplitList 函数，将文章信息分割成与每个查询匹配的组\n    retrieved_articles = SplitList(articles_flatten, top_per_query)\n    return retrieved_articles  # 返回检索到的文章列表\n\n\n# 定义一个函数，从验证数据集中检索相关的文档\ndef get_relevant_documents(df_valid):\n    df_chunk_size=800  # 设置每个数据块的大小\n    \n    # 从指定路径加载预处理过的数据集\n    cohere_dataset_filtered = load_from_disk(\"/kaggle/working/stem-wiki-cohere-no-emb\")\n    # 修改文本，将标题和文本合并，并进行Unicode标准化（NFKD），同时替换双引号\n    modified_texts = cohere_dataset_filtered.map(lambda example:\n                                             {'temp_text':\n                                              unicodedata.normalize(\"NFKD\", f\"{example['title']} {example['text']}\").replace('\"',\"\")},\n                                             num_proc=2)[\"temp_text\"]\n    \n    # 初始化两个列表，用于存储文章的索引和值\n    all_articles_indices = []\n    all_articles_values = []\n    # 使用 tqdm 进度条，遍历验证数据集，每次处理一个数据块\n    for idx in tqdm(range(0, df_valid.shape[0], df_chunk_size)):\n        df_valid_ = df_valid.iloc[idx: idx+df_chunk_size]  # 获取当前数据块\n    \n        # 调用 retrieval 函数，获取相关文章的索引和得分\n        articles_indices, merged_top_scores = retrieval(df_valid_, modified_texts)\n        # 将结果添加到列表中\n        all_articles_indices.append(articles_indices)\n        all_articles_values.append(merged_top_scores)\n        \n    # 将所有结果连接成数组\n    article_indices_array = np.concatenate(all_articles_indices, axis=0)\n    articles_values_array = np.concatenate(all_articles_values, axis=0).reshape(-1)\n    \n    # 获取每个查询的文章数量\n    top_per_query = article_indices_array.shape[1]\n    # 展平文章信息，并按查询分组\n    articles_flatten = [(\n                         articles_values_array[index],\n                         cohere_dataset_filtered[idx.item()][\"title\"],\n                         unicodedata.normalize(\"NFKD\", cohere_dataset_filtered[idx.item()][\"text\"]),\n                        )\n                        for index,idx in enumerate(article_indices_array.reshape(-1))]\n    # 使用 SplitList 函数，将文章信息分割成与每个查询匹配的组\n    retrieved_articles = SplitList(articles_flatten, top_per_query)\n    return retrieved_articles  # 返回检索到的文章列表\n\n\n\n# 定义一个检索函数，输入为验证数据集和修改过的文本\ndef retrieval(df_valid, modified_texts):\n    # 将验证集的每行合并成一个字符串，包括问题和答案选项\n    corpus_df_valid = df_valid.apply(lambda row:\n                                     f'{row[\"prompt\"]}\\n{row[\"prompt\"]}\\n{row[\"prompt\"]}\\n{row[\"A\"]}\\n{row[\"B\"]}\\n{row[\"C\"]}\\n{row[\"D\"]}\\n{row[\"E\"]}',\n                                     axis=1).values\n    # 初始化第一个TF-IDF向量化器\n    vectorizer1 = TfidfVectorizer(ngram_range=(1,2),\n                                 token_pattern=r\"(?u)\\b[\\w/.-]+\\b|!|/|\\?|\\\"|\\'\",\n                                 stop_words=stop_words)\n    # 在验证集文本上拟合第一个向量化器\n    vectorizer1.fit(corpus_df_valid)\n    # 获取验证集的词汇表\n    vocab_df_valid = vectorizer1.get_feature_names_out()\n    # 使用验证集的词汇表初始化第二个TF-IDF向量化器\n    vectorizer = TfidfVectorizer(ngram_range=(1,2),\n                                 token_pattern=r\"(?u)\\b[\\w/.-]+\\b|!|/|\\?|\\\"|\\'\",\n                                 stop_words=stop_words,\n                                 vocabulary=vocab_df_valid)\n    # 在修改过的文本上拟合第二个向量化器\n    vectorizer.fit(modified_texts[:500000])\n    # 转换验证集文本为TF-IDF向量\n    corpus_tf_idf = vectorizer.transform(corpus_df_valid)\n    \n    # 打印向量化器词汇表的长度\n    print(f\"length of vectorizer vocab is {len(vectorizer.get_feature_names_out())}\")\n\n    chunk_size = 100000  # 设置每个数据块的大小\n    top_per_chunk = 15   # 设置每个数据块的顶部文档数\n    top_per_query = 15   # 设置每个查询的顶部文档数\n\n    # 初始化列表以保存每个数据块的顶部索引和值\n    all_chunk_top_indices = []\n    all_chunk_top_values = []\n\n    # 使用 tqdm 进度条，遍历修改过的文本，每次处理一个数据块\n    for idx in tqdm(range(0, len(modified_texts), chunk_size)):\n        # 将当前数据块的文本转换为TF-IDF向量\n        wiki_vectors = vectorizer.transform(modified_texts[idx: idx+chunk_size])\n        # 计算验证集和当前数据块的文档之间的余弦相似度\n        temp_scores = (corpus_tf_idf * wiki_vectors.T).toarray()\n        # 获取每个查询的顶部文档索引和值\n        chunk_top_indices = temp_scores.argpartition(-top_per_chunk, axis=1)[:, -top_per_chunk:]\n        chunk_top_values = temp_scores[np.arange(temp_scores.shape[0])[:, np.newaxis], chunk_top_indices]\n\n        # 将结果添加到列表中\n        all_chunk_top_indices.append(chunk_top_indices + idx)\n        all_chunk_top_values.append(chunk_top_values)\n\n    # 将所有结果连接成数组\n    top_indices_array = np.concatenate(all_chunk_top_indices, axis=1)\n    top_values_array = np.concatenate(all_chunk_top_values, axis=1)\n    \n    # 获取每个查询的顶部文档得分和索引\n    merged_top_scores = np.sort(top_values_array, axis=1)[:,-top_per_query:]\n    merged_top_indices = top_values_array.argsort(axis=1)[:,-top_per_query:]\n    # 获取文章的索引\n    articles_indices = top_indices_array[np.arange(top_indices_array.shape[0])[:, np.newaxis], merged_top_indices]\n    \n    # 返回文章的索引和得分\n    return articles_indices, merged_top_scores\n\n\n# 函数定义：准备用于模型的问答输入\ndef prepare_answering_input(\n        tokenizer,        # 令牌化器，用于将文本转换为令牌\n        question,         # 提供的问题字符串\n        options,          # 提供的选项列表\n        context,          # 提供的上下文字符串\n        max_seq_length=4096, # 最大序列长度，默认为4096\n    ):\n    # 拼接上下文、令牌化器的起始令牌和问题\n    c_plus_q   = context + ' ' + tokenizer.bos_token + ' ' + question\n    # 创建一个列表，包含与选项数量相同的拼接字符串\n    c_plus_q_4 = [c_plus_q] * len(options)\n    # 使用令牌化器处理拼接字符串和选项，生成令牌化的示例\n    tokenized_examples = tokenizer(\n        c_plus_q_4, options,\n        max_length=max_seq_length,  # 指定最大长度\n        padding=\"longest\",          # 填充至最长示例的长度\n        truncation=False,           # 不截断\n        return_tensors=\"pt\",        # 返回PyTorch张量\n    )\n    # 从令牌化的示例中获取输入ID，并增加一个维度\n    input_ids = tokenized_examples['input_ids'].unsqueeze(0)\n    # 从令牌化的示例中获取注意力掩码，并增加一个维度\n    attention_mask = tokenized_examples['attention_mask'].unsqueeze(0)\n    # 创建一个字典，包含输入ID和注意力掩码，同时将它们移动到模型的设备上\n    example_encoded = {\n        \"input_ids\": input_ids.to(model.device.index),\n        \"attention_mask\": attention_mask.to(model.device.index),\n    }\n    # 返回编码后的示例\n    return example_encoded","metadata":{"execution":{"iopub.status.busy":"2023-10-07T12:38:55.738686Z","iopub.execute_input":"2023-10-07T12:38:55.739055Z","iopub.status.idle":"2023-10-07T12:38:55.761062Z","shell.execute_reply.started":"2023-10-07T12:38:55.739019Z","shell.execute_reply":"2023-10-07T12:38:55.760172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_words = ['each', 'you', 'the', 'use', 'used',\n                  'where', 'themselves', 'nor', \"it's\", 'how', \"don't\", 'just', 'your',\n                  'about', 'himself', 'with', \"weren't\", 'hers', \"wouldn't\", 'more', 'its', 'were',\n                  'his', 'their', 'then', 'been', 'myself', 're', 'not',\n                  'ours', 'will', 'needn', 'which', 'here', 'hadn', 'it', 'our', 'there', 'than',\n                  'most', \"couldn't\", 'both', 'some', 'for', 'up', 'couldn', \"that'll\",\n                  \"she's\", 'over', 'this', 'now', 'until', 'these', 'few', 'haven',\n                  'of', 'wouldn', 'into', 'too', 'to', 'very', 'shan', 'before', 'the', 'they',\n                  'between', \"doesn't\", 'are', 'was', 'out', 'we', 'me',\n                  'after', 'has', \"isn't\", 'have', 'such', 'should', 'yourselves', 'or', 'during', 'herself',\n                  'doing', 'in', \"shouldn't\", \"won't\", 'when', 'do', 'through', 'she',\n                  'having', 'him', \"haven't\", 'against', 'itself', 'that',\n                  'did', 'theirs', 'can', 'those',\n                  'own', 'so', 'and', 'who', \"you've\", 'yourself', 'her', 'he', 'only',\n                  'what', 'ourselves', 'again', 'had', \"you'd\", 'is', 'other',\n                  'why', 'while', 'from', 'them', 'if', 'above', 'does', 'whom',\n                  'yours', 'but', 'being', \"wasn't\", 'be']","metadata":{"execution":{"iopub.status.busy":"2023-10-07T12:38:55.762644Z","iopub.execute_input":"2023-10-07T12:38:55.763327Z","iopub.status.idle":"2023-10-07T12:38:55.780584Z","shell.execute_reply.started":"2023-10-07T12:38:55.763296Z","shell.execute_reply":"2023-10-07T12:38:55.779556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_valid = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-10-07T12:38:55.782025Z","iopub.execute_input":"2023-10-07T12:38:55.782574Z","iopub.status.idle":"2023-10-07T12:38:55.816833Z","shell.execute_reply.started":"2023-10-07T12:38:55.782542Z","shell.execute_reply":"2023-10-07T12:38:55.8159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"retrieved_articles_parsed = get_relevant_documents_parsed(df_valid)\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-10-07T12:38:55.818172Z","iopub.execute_input":"2023-10-07T12:38:55.818805Z","iopub.status.idle":"2023-10-07T12:46:27.333365Z","shell.execute_reply.started":"2023-10-07T12:38:55.818775Z","shell.execute_reply":"2023-10-07T12:46:27.33235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"retrieved_articles = get_relevant_documents(df_valid)\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-10-07T12:46:27.334826Z","iopub.execute_input":"2023-10-07T12:46:27.335815Z","iopub.status.idle":"2023-10-07T12:55:27.405661Z","shell.execute_reply.started":"2023-10-07T12:46:27.335779Z","shell.execute_reply":"2023-10-07T12:55:27.404609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer_dir = \"/kaggle/input/llm-fintune3/LLM_fintune3/LLM_model2/tokenizer\"\ntokenizer = AutoTokenizer.from_pretrained(tokenizer_dir)\nconfig = torch.load(\"/kaggle/input/llm-fintune3/LLM_fintune3/LLM_model2/config.pth\")\nmodel = AutoModelForMultipleChoice.from_config(config)\n\nstate = torch.load(f\"/kaggle/input/llm-fintune3/LLM_fintune3/LLM_model2/pytorch_model.bin\",\n                       map_location=torch.device('cpu'))\nmodel.load_state_dict(state)\nmodel = model.cuda()\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2023-10-07T12:55:27.409166Z","iopub.execute_input":"2023-10-07T12:55:27.409915Z","iopub.status.idle":"2023-10-07T12:55:51.327306Z","shell.execute_reply.started":"2023-10-07T12:55:27.409891Z","shell.execute_reply":"2023-10-07T12:55:51.326308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\")\ntest_df['context'] = [f\"{retrieved_article[-15][2]}\\n{retrieved_article[-14][2]}\\n{retrieved_article[-13][2]}\\n{retrieved_article[-12][2]}\\n{retrieved_article[-11][2]}\\n{retrieved_article[-10][2]}\\n{retrieved_article[-9][2]}\\n{retrieved_article[-8][2]}\\n{retrieved_article[-7][2]}\\n{retrieved_article[-6][2]}\\n{retrieved_article[-5][2]}\\n{retrieved_article[-4][2]}\\n{retrieved_article[-3][2]}\\n{retrieved_article[-2][2]}\\n{retrieved_article[-1][2]}\" for retrieved_article in retrieved_articles]\n\ntest_df.index = list(range(len(test_df)))\ntest_df['id'] = list(range(len(test_df)))\ntest_df[\"prompt\"] = test_df[\"context\"].apply(lambda x: x[-4820:]) + \" #### \" +  test_df[\"prompt\"]\ntest_df['answer'] = 'A'\n\n# We'll create a dictionary to convert option names (A, B, C, D, E) into indices and back again\noptions = 'ABCDE'\nindices = list(range(5))\n\noption_to_index = {option: index for option, index in zip(options, indices)}\nindex_to_option = {index: option for option, index in zip(options, indices)}\n\ndef preprocess(example):\n    # The AutoModelForMultipleChoice class expects a set of question/answer pairs\n    # so we'll copy our question 5 times before tokenizing\n    first_sentence = [example['prompt']] * 5\n    second_sentence = []\n    for option in options:\n        second_sentence.append(example[option])\n    # Our tokenizer will turn our text into token IDs BERT can understand\n    tokenized_example = tokenizer(first_sentence, second_sentence, truncation=True)\n    tokenized_example['label'] = option_to_index[example['answer']]\n    return tokenized_example\n@dataclass\nclass DataCollatorForMultipleChoice:\n    tokenizer: PreTrainedTokenizerBase\n    padding: Union[bool, str, PaddingStrategy] = True\n    max_length: Optional[int] = None\n    pad_to_multiple_of: Optional[int] = None\n    \n    def __call__(self, features):\n        label_name = \"label\" if 'label' in features[0].keys() else 'labels'\n        labels = [feature.pop(label_name) for feature in features]\n        batch_size = len(features)\n        num_choices = len(features[0]['input_ids'])\n        flattened_features = [\n            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n        ]\n        flattened_features = sum(flattened_features, [])\n        \n        batch = self.tokenizer.pad(\n            flattened_features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors='pt',\n        )\n        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n        return batch\ntokenized_test_dataset = Dataset.from_pandas(test_df[['id', 'prompt', 'A', 'B', 'C', 'D', 'E', 'answer']].drop(columns=['id'])).map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])\ntokenized_test_dataset = tokenized_test_dataset.remove_columns([\"__index_level_0__\"])\ndata_collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\ntest_dataloader = DataLoader(tokenized_test_dataset, batch_size=1, shuffle=False, collate_fn=data_collator)\ntest_predictionsj = []\nfor batch in test_dataloader:\n    for k in batch.keys():\n        batch[k] = batch[k].cuda()\n    with torch.no_grad():\n        outputs = model(**batch)\n    test_predictionsj.append(outputs.logits.cpu().detach())\n\ntest_predictionsj = torch.cat(test_predictionsj)","metadata":{"execution":{"iopub.status.busy":"2023-10-07T12:55:51.328835Z","iopub.execute_input":"2023-10-07T12:55:51.329424Z","iopub.status.idle":"2023-10-07T12:57:12.510572Z","shell.execute_reply.started":"2023-10-07T12:55:51.32939Z","shell.execute_reply":"2023-10-07T12:57:12.509443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\")\ntest_df['context'] = [f\"{retrieved_articles_parse[-15][2]}\\n{retrieved_articles_parse[-14][2]}\\n{retrieved_articles_parse[-13][2]}\\n{retrieved_articles_parse[-12][2]}\\n{retrieved_articles_parse[-11][2]}\\n{retrieved_articles_parse[-10][2]}\\n{retrieved_articles_parse[-9][2]}\\n{retrieved_articles_parse[-8][2]}\\n{retrieved_articles_parse[-7][2]}\\n{retrieved_articles_parse[-6][2]}\\n{retrieved_articles_parse[-5][2]}\\n{retrieved_articles_parse[-4][2]}\\n{retrieved_articles_parse[-3][2]}\\n{retrieved_articles_parse[-2][2]}\\n{retrieved_articles_parse[-1][2]}\" for retrieved_articles_parse in retrieved_articles_parsed]\n\ntest_df.index = list(range(len(test_df)))\ntest_df['id'] = list(range(len(test_df)))\ntest_df[\"prompt\"] = test_df[\"context\"].apply(lambda x: x[-4820:]) + \" #### \" +  test_df[\"prompt\"]\ntest_df['answer'] = 'A'\n# We'll create a dictionary to convert option names (A, B, C, D, E) into indices and back again\noptions = 'ABCDE'\nindices = list(range(5))\n\noption_to_index = {option: index for option, index in zip(options, indices)}\nindex_to_option = {index: option for option, index in zip(options, indices)}\n\ndef preprocess(example):\n    # The AutoModelForMultipleChoice class expects a set of question/answer pairs\n    # so we'll copy our question 5 times before tokenizing\n    first_sentence = [example['prompt']] * 5\n    second_sentence = []\n    for option in options:\n        second_sentence.append(example[option])\n    # Our tokenizer will turn our text into token IDs BERT can understand\n    tokenized_example = tokenizer(first_sentence, second_sentence, truncation=True)\n    tokenized_example['label'] = option_to_index[example['answer']]\n    return tokenized_example\n@dataclass\nclass DataCollatorForMultipleChoice:\n    tokenizer: PreTrainedTokenizerBase\n    padding: Union[bool, str, PaddingStrategy] = True\n    max_length: Optional[int] = None\n    pad_to_multiple_of: Optional[int] = None\n    \n    def __call__(self, features):\n        label_name = \"label\" if 'label' in features[0].keys() else 'labels'\n        labels = [feature.pop(label_name) for feature in features]\n        batch_size = len(features)\n        num_choices = len(features[0]['input_ids'])\n        flattened_features = [\n            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n        ]\n        flattened_features = sum(flattened_features, [])\n        \n        batch = self.tokenizer.pad(\n            flattened_features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors='pt',\n        )\n        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n        return batch\ntokenized_test_dataset = Dataset.from_pandas(test_df[['id', 'prompt', 'A', 'B', 'C', 'D', 'E', 'answer']].drop(columns=['id'])).map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])\ntokenized_test_dataset = tokenized_test_dataset.remove_columns([\"__index_level_0__\"])\ndata_collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\ntest_dataloader = DataLoader(tokenized_test_dataset, batch_size=1, shuffle=False, collate_fn=data_collator)\ntest_predictionsi = []\nfor batch in test_dataloader:\n    for k in batch.keys():\n        batch[k] = batch[k].cuda()\n    with torch.no_grad():\n        outputs = model(**batch)\n    test_predictionsi.append(outputs.logits.cpu().detach())\n\ntest_predictionsi = torch.cat(test_predictionsi)","metadata":{"execution":{"iopub.status.busy":"2023-10-07T12:57:12.51229Z","iopub.execute_input":"2023-10-07T12:57:12.512638Z","iopub.status.idle":"2023-10-07T12:58:32.608154Z","shell.execute_reply.started":"2023-10-07T12:57:12.512607Z","shell.execute_reply":"2023-10-07T12:58:32.607079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_dir = \"/kaggle/input/llm-fintune3/LLM_fintune3/LLM_model1\"\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\nmodel = AutoModelForMultipleChoice.from_pretrained(model_dir).cuda()\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2023-10-07T12:58:32.609677Z","iopub.execute_input":"2023-10-07T12:58:32.610275Z","iopub.status.idle":"2023-10-07T12:58:48.757282Z","shell.execute_reply.started":"2023-10-07T12:58:32.610242Z","shell.execute_reply":"2023-10-07T12:58:48.756306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\")\ntest_df['context'] = [f\"{retrieved_article[-15][2]}\\n{retrieved_article[-14][2]}\\n{retrieved_article[-13][2]}\\n{retrieved_article[-12][2]}\\n{retrieved_article[-11][2]}\\n{retrieved_article[-10][2]}\\n{retrieved_article[-9][2]}\\n{retrieved_article[-8][2]}\\n{retrieved_article[-7][2]}\\n{retrieved_article[-6][2]}\\n{retrieved_article[-5][2]}\\n{retrieved_article[-4][2]}\\n{retrieved_article[-3][2]}\\n{retrieved_article[-2][2]}\\n{retrieved_article[-1][2]}\" for retrieved_article in retrieved_articles]\n\ntest_df.index = list(range(len(test_df)))\ntest_df['id'] = list(range(len(test_df)))\ntest_df[\"prompt\"] = test_df[\"context\"].apply(lambda x: x[-4820:]) + \" #### \" +  test_df[\"prompt\"]\ntest_df['answer'] = 'A'\n\n# We'll create a dictionary to convert option names (A, B, C, D, E) into indices and back again\noptions = 'ABCDE'\nindices = list(range(5))\n\noption_to_index = {option: index for option, index in zip(options, indices)}\nindex_to_option = {index: option for option, index in zip(options, indices)}\n\ndef preprocess(example):\n    # The AutoModelForMultipleChoice class expects a set of question/answer pairs\n    # so we'll copy our question 5 times before tokenizing\n    first_sentence = [example['prompt']] * 5\n    second_sentence = []\n    for option in options:\n        second_sentence.append(example[option])\n    # Our tokenizer will turn our text into token IDs BERT can understand\n    tokenized_example = tokenizer(first_sentence, second_sentence, truncation=True)\n    tokenized_example['label'] = option_to_index[example['answer']]\n    return tokenized_example\n@dataclass\nclass DataCollatorForMultipleChoice:\n    tokenizer: PreTrainedTokenizerBase\n    padding: Union[bool, str, PaddingStrategy] = True\n    max_length: Optional[int] = None\n    pad_to_multiple_of: Optional[int] = None\n    \n    def __call__(self, features):\n        label_name = \"label\" if 'label' in features[0].keys() else 'labels'\n        labels = [feature.pop(label_name) for feature in features]\n        batch_size = len(features)\n        num_choices = len(features[0]['input_ids'])\n        flattened_features = [\n            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n        ]\n        flattened_features = sum(flattened_features, [])\n        \n        batch = self.tokenizer.pad(\n            flattened_features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors='pt',\n        )\n        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n        return batch\ntokenized_test_dataset = Dataset.from_pandas(test_df[['id', 'prompt', 'A', 'B', 'C', 'D', 'E', 'answer']].drop(columns=['id'])).map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])\ntokenized_test_dataset = tokenized_test_dataset.remove_columns([\"__index_level_0__\"])\ndata_collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\ntest_dataloader = DataLoader(tokenized_test_dataset, batch_size=1, shuffle=False, collate_fn=data_collator)\ntest_predictionsk = []\nfor batch in test_dataloader:\n    for k in batch.keys():\n        batch[k] = batch[k].cuda()\n    with torch.no_grad():\n        outputs = model(**batch)\n    test_predictionsk.append(outputs.logits.cpu().detach())\n\ntest_predictionsk = torch.cat(test_predictionsk)","metadata":{"execution":{"iopub.status.busy":"2023-10-07T12:58:48.758876Z","iopub.execute_input":"2023-10-07T12:58:48.759256Z","iopub.status.idle":"2023-10-07T13:00:08.135125Z","shell.execute_reply.started":"2023-10-07T12:58:48.759222Z","shell.execute_reply":"2023-10-07T13:00:08.134094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\")\ntest_df['context'] = [f\"{retrieved_articles_parse[-15][2]}\\n{retrieved_articles_parse[-14][2]}\\n{retrieved_articles_parse[-13][2]}\\n{retrieved_articles_parse[-12][2]}\\n{retrieved_articles_parse[-11][2]}\\n{retrieved_articles_parse[-10][2]}\\n{retrieved_articles_parse[-9][2]}\\n{retrieved_articles_parse[-8][2]}\\n{retrieved_articles_parse[-7][2]}\\n{retrieved_articles_parse[-6][2]}\\n{retrieved_articles_parse[-5][2]}\\n{retrieved_articles_parse[-4][2]}\\n{retrieved_articles_parse[-3][2]}\\n{retrieved_articles_parse[-2][2]}\\n{retrieved_articles_parse[-1][2]}\" for retrieved_articles_parse in retrieved_articles_parsed]\n\ntest_df.index = list(range(len(test_df)))\ntest_df['id'] = list(range(len(test_df)))\ntest_df[\"prompt\"] = test_df[\"context\"].apply(lambda x: x[-4820:]) + \" #### \" +  test_df[\"prompt\"]\ntest_df['answer'] = 'A'\n# We'll create a dictionary to convert option names (A, B, C, D, E) into indices and back again\noptions = 'ABCDE'\nindices = list(range(5))\n\noption_to_index = {option: index for option, index in zip(options, indices)}\nindex_to_option = {index: option for option, index in zip(options, indices)}\n\ndef preprocess(example):\n    # The AutoModelForMultipleChoice class expects a set of question/answer pairs\n    # so we'll copy our question 5 times before tokenizing\n    first_sentence = [example['prompt']] * 5\n    second_sentence = []\n    for option in options:\n        second_sentence.append(example[option])\n    # Our tokenizer will turn our text into token IDs BERT can understand\n    tokenized_example = tokenizer(first_sentence, second_sentence, truncation=True)\n    tokenized_example['label'] = option_to_index[example['answer']]\n    return tokenized_example\n@dataclass\nclass DataCollatorForMultipleChoice:\n    tokenizer: PreTrainedTokenizerBase\n    padding: Union[bool, str, PaddingStrategy] = True\n    max_length: Optional[int] = None\n    pad_to_multiple_of: Optional[int] = None\n    \n    def __call__(self, features):\n        label_name = \"label\" if 'label' in features[0].keys() else 'labels'\n        labels = [feature.pop(label_name) for feature in features]\n        batch_size = len(features)\n        num_choices = len(features[0]['input_ids'])\n        flattened_features = [\n            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n        ]\n        flattened_features = sum(flattened_features, [])\n        \n        batch = self.tokenizer.pad(\n            flattened_features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors='pt',\n        )\n        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n        return batch\ntokenized_test_dataset = Dataset.from_pandas(test_df[['id', 'prompt', 'A', 'B', 'C', 'D', 'E', 'answer']].drop(columns=['id'])).map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])\ntokenized_test_dataset = tokenized_test_dataset.remove_columns([\"__index_level_0__\"])\ndata_collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\ntest_dataloader = DataLoader(tokenized_test_dataset, batch_size=1, shuffle=False, collate_fn=data_collator)\ntest_predictionsl = []\nfor batch in test_dataloader:\n    for k in batch.keys():\n        batch[k] = batch[k].cuda()\n    with torch.no_grad():\n        outputs = model(**batch)\n    test_predictionsl.append(outputs.logits.cpu().detach())\n\ntest_predictionsl = torch.cat(test_predictionsl)","metadata":{"execution":{"iopub.status.busy":"2023-10-07T13:00:08.136603Z","iopub.execute_input":"2023-10-07T13:00:08.137223Z","iopub.status.idle":"2023-10-07T13:01:28.303101Z","shell.execute_reply.started":"2023-10-07T13:00:08.137186Z","shell.execute_reply":"2023-10-07T13:01:28.302043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_predictions = (test_predictionsi+test_predictionsj+test_predictionsk+test_predictionsl)/4.0","metadata":{"execution":{"iopub.status.busy":"2023-10-07T13:01:28.304705Z","iopub.execute_input":"2023-10-07T13:01:28.305044Z","iopub.status.idle":"2023-10-07T13:01:28.312742Z","shell.execute_reply.started":"2023-10-07T13:01:28.305013Z","shell.execute_reply":"2023-10-07T13:01:28.311704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_as_ids = np.argsort(-test_predictions, 1)\n\npredictions_as_answer_letters = np.array(list('ABCDE'))[predictions_as_ids]\n# predictions_as_answer_letters[:3]\n\npredictions_as_string = test_df['prediction'] = [\n    ' '.join(row) for row in predictions_as_answer_letters[:, :3]\n]\n\nsubmission = test_df[['id', 'prediction']]\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-10-07T13:01:28.314148Z","iopub.execute_input":"2023-10-07T13:01:28.314701Z","iopub.status.idle":"2023-10-07T13:01:28.421398Z","shell.execute_reply.started":"2023-10-07T13:01:28.314671Z","shell.execute_reply":"2023-10-07T13:01:28.42054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}