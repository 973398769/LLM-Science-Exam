{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# How To Train Model for Open Book Q&A Technique - Part 2\nThe notebook you are reading is a fork of Mgoksu's great notebook [here][1]. Mgoksu (@mgoksu) demonstrated how to achieve top public LB=0.807 using Open Book technique. The Open Book method was first presented by JJ (@jjinho) [here][2], then Quangteo (@quangbk) improved RAM usage [here][3], and Anil (@nlztrk) combined with Q&A [here][4]. Radek (@radek1) demonstrated the strength of Q&A [here][5].\n\nIn my previous notebook [here][6] (i.e. Part 1), we demonstrated how to train a model for Open Book. The model was trained using my 60k Kaggle dataset [here][7]. If you enjoy the notebook you are reading, please upvote the dataset too. Thanks!\n\nIn this notebook, we will load the trained model output from my previous notebook. We will infer this model after running the code from Mgoksu's public notebook to use Open Book to seach Wikipedia for context. For each test sample in the hidden dataset, we will append Wikipedia context. Then our trained model will infer the multiple choice answer (using both question and appended Wikipedia context). When predicting the answer, this notebook uses a 50% 50% ensemble of the new Q&A model we trained ensembled with Mgoksu's original model. Here is a diagram showing the Open Book method:\n\n![](https://miro.medium.com/v2/resize:fit:800/format:webp/1*bTGY3fKIgNefQxNsOYpnBw.png)\n\n(image source [here][8])\n\n[1]: https://www.kaggle.com/code/mgoksu/0-807-sharing-my-trained-with-context-model\n[2]: https://www.kaggle.com/code/jjinho/open-book-llm-science-exam\n[3]: https://www.kaggle.com/code/quangbk/open-book-llm-science-exam-reduced-ram-usage\n[4]: https://www.kaggle.com/code/nlztrk/openbook-debertav3-large-baseline-single-model\n[5]: https://www.kaggle.com/code/radek1/new-dataset-deberta-v3-large-training\n[6]: https://www.kaggle.com/code/cdeotte/how-to-train-open-book-model\n[7]: https://www.kaggle.com/datasets/cdeotte/60k-data-with-context-v2\n[8]: https://blog.gopenai.com/enrich-llms-with-retrieval-augmented-generation-rag-17b82a96b6f0","metadata":{}},{"cell_type":"markdown","source":"# OpenBook DeBERTaV3-Large with an updated model\n\nThis work is based on the great [work](https://www.kaggle.com/code/nlztrk/openbook-debertav3-large-baseline-single-model) of [nlztrk](https://www.kaggle.com/nlztrk).\n\nI trained a model offline using the dataset I shared [here](https://www.kaggle.com/datasets/mgoksu/llm-science-exam-dataset-w-context). I just added my model to the original notebook. The model is available [here](https://www.kaggle.com/datasets/mgoksu/llm-science-run-context-2).\n\nI also addressed the problem of [CSV Not Found at submission](https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/434228) with this notebook by clipping the context like so:\n\n`test_df[\"prompt\"] = test_df[\"context\"].apply(lambda x: x[:1500]) + \" #### \" +  test_df[\"prompt\"]`\n\nYou can probably get more than 1500 without getting an OOM.","metadata":{}},{"cell_type":"code","source":"# installing offline dependencies\n!pip install -U /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n!cp -rf /kaggle/input/sentence-transformers-222/sentence-transformers /kaggle/working/sentence-transformers\n!pip install -U /kaggle/working/sentence-transformers\n!pip install -U /kaggle/input/blingfire-018/blingfire-0.1.8-py3-none-any.whl\n\n!pip install --no-index --no-deps /kaggle/input/llm-whls/transformers-4.31.0-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/llm-whls/peft-0.4.0-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/llm-whls/datasets-2.14.3-py3-none-any.whl\n!pip install --no-index --no-deps /kaggle/input/llm-whls/trl-0.5.0-py3-none-any.whl","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":false,"_kg_hide-output":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":126.809817,"end_time":"2023-08-14T10:09:22.925969","exception":false,"start_time":"2023-08-14T10:07:16.116152","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-24T08:44:29.434598Z","iopub.execute_input":"2023-08-24T08:44:29.435147Z","iopub.status.idle":"2023-08-24T08:46:34.4289Z","shell.execute_reply.started":"2023-08-24T08:44:29.43511Z","shell.execute_reply":"2023-08-24T08:46:34.427748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport pandas as pd\nimport numpy as np\nimport re\nfrom tqdm.auto import tqdm\nimport blingfire as bf\nfrom __future__ import annotations\n\nfrom collections.abc import Iterable\n\nimport faiss\nfrom faiss import write_index, read_index\n\nfrom sentence_transformers import SentenceTransformer\n\nimport torch\nimport ctypes\nlibc = ctypes.CDLL(\"libc.so.6\")\n\nfrom dataclasses import dataclass\nfrom typing import Optional, Union\n\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\nfrom torch.utils.data import DataLoader","metadata":{"papermill":{"duration":8.534957,"end_time":"2023-08-14T10:09:31.474781","exception":false,"start_time":"2023-08-14T10:09:22.939824","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-24T08:46:34.432022Z","iopub.execute_input":"2023-08-24T08:46:34.433066Z","iopub.status.idle":"2023-08-24T08:46:48.304439Z","shell.execute_reply.started":"2023-08-24T08:46:34.433025Z","shell.execute_reply":"2023-08-24T08:46:48.303462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 检索管道\n# 这个函数把文档划分成一个一个句子，划分成句子之后我们可以把句子拿到我们的样本里面进行推理，处理成一个句子的模式\ndef process_documents(documents: Iterable[str],\n                      document_ids: Iterable,\n                      split_sentences: bool = True,\n                      filter_len: int = 3,\n                      disable_progress_bar: bool = False) -> pd.DataFrame:\n    \"\"\"\n    主要的辅助函数，用于处理来自电子病历（EMR）的文档。\n    \n    :param documents: 包含文档的可迭代对象，其中文档是字符串\n    :param document_ids: 包含文档唯一标识符的可迭代对象\n    :param split_sentences: 标志，用于确定是否将部分进一步分成句子\n    :param filter_len: 句子的最小字符长度（否则过滤掉）\n    :param disable_progress_bar: 标志，用于禁用tqdm进度条\n    :return: 包含`document_id`、`text`、`section`、`offset`列的Pandas DataFrame\n    \"\"\"\n    \n    df = sectionize_documents(documents, document_ids, disable_progress_bar)\n\n    if split_sentences:\n        df = sentencize(df.text.values, \n                        df.document_id.values,\n                        df.offset.values, \n                        filter_len, \n                        disable_progress_bar)\n    return df\n\n\ndef sectionize_documents(documents: Iterable[str],\n                         document_ids: Iterable,\n                         disable_progress_bar: bool = False) -> pd.DataFrame:\n    \"\"\"\n    获取影像报告的各个部分，并仅返回选定的部分（默认为FINDINGS、IMPRESSION和ADDENDUM）。\n\n    :param documents: 包含文档的可迭代对象，其中文档是字符串\n    :param document_ids: 包含文档唯一标识符的可迭代对象\n    :param disable_progress_bar: 标志，用于禁用tqdm进度条\n    :return: 包含`document_id`、`text`、`offset`列的Pandas DataFrame\n    \"\"\"\n    processed_documents = []\n    for document_id, document in tqdm(zip(document_ids, documents), total=len(documents), disable=disable_progress_bar):\n        row = {}\n        text, start, end = (document, 0, len(document))\n        row['document_id'] = document_id\n        row['text'] = text\n        row['offset'] = (start, end)\n\n        processed_documents.append(row)\n\n    _df = pd.DataFrame(processed_documents)\n    if _df.shape[0] > 0:\n        return _df.sort_values(['document_id', 'offset']).reset_index(drop=True)\n    else:\n        return _df\n\n\ndef sentencize(documents: Iterable[str],\n               document_ids: Iterable,\n               offsets: Iterable[tuple[int, int]],\n               filter_len: int = 3,\n               disable_progress_bar: bool = False) -> pd.DataFrame:\n    \"\"\"\n    将文档分成句子。可以与`sectionize_documents`一起使用，以将文档进一步分成更易管理的片段。\n    接受偏移量以确保在拆分后，句子可以与原始文档中的位置匹配。\n\n    :param documents: 包含文档的可迭代对象，其中文档是字符串\n    :param document_ids: 包含文档唯一标识符的可迭代对象\n    :param offsets: 可迭代的元组，表示开始和结束索引\n    :param filter_len: 句子的最小字符长度（否则过滤掉）\n    :return: 包含`document_id`、`text`、`section`、`offset`列的Pandas DataFrame\n    \"\"\"\n\n    document_sentences = []\n    for document, document_id, offset in tqdm(zip(documents, document_ids, offsets), total=len(documents), disable=disable_progress_bar):\n        try:\n            _, sentence_offsets = bf.text_to_sentences_and_offsets(document)\n            for o in sentence_offsets:\n                if o[1]-o[0] > filter_len:\n                    sentence = document[o[0]:o[1]]\n                    abs_offsets = (o[0]+offset[0], o[1]+offset[0])\n                    row = {}\n                    row['document_id'] = document_id\n                    row['text'] = sentence\n                    row['offset'] = abs_offsets\n                    document_sentences.append(row)\n        except:\n            continue\n    return pd.DataFrame(document_sentences)","metadata":{"papermill":{"duration":0.034054,"end_time":"2023-08-14T10:09:31.574046","exception":false,"start_time":"2023-08-14T10:09:31.539992","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-24T08:46:48.305967Z","iopub.execute_input":"2023-08-24T08:46:48.306332Z","iopub.status.idle":"2023-08-24T08:46:48.323357Z","shell.execute_reply.started":"2023-08-24T08:46:48.306298Z","shell.execute_reply":"2023-08-24T08:46:48.322284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SIM_MODEL = '/kaggle/input/sentencetransformers-allminilml6v2/sentence-transformers_all-MiniLM-L6-v2'\nDEVICE = 0\nMAX_LENGTH = 384\nBATCH_SIZE = 16\n\nWIKI_PATH = \"/kaggle/input/wikipedia-20230701\"\nwiki_files = os.listdir(WIKI_PATH)","metadata":{"papermill":{"duration":0.036342,"end_time":"2023-08-14T10:09:31.623595","exception":false,"start_time":"2023-08-14T10:09:31.587253","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-24T08:46:48.326547Z","iopub.execute_input":"2023-08-24T08:46:48.326889Z","iopub.status.idle":"2023-08-24T08:46:48.351236Z","shell.execute_reply.started":"2023-08-24T08:46:48.326857Z","shell.execute_reply":"2023-08-24T08:46:48.350127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Relevant Title Retrieval","metadata":{}},{"cell_type":"code","source":"trn = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\").drop(\"id\", 1)\ntrn.head()","metadata":{"papermill":{"duration":0.058533,"end_time":"2023-08-14T10:09:31.695383","exception":false,"start_time":"2023-08-14T10:09:31.63685","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-24T08:46:48.352478Z","iopub.execute_input":"2023-08-24T08:46:48.352916Z","iopub.status.idle":"2023-08-24T08:46:48.388993Z","shell.execute_reply.started":"2023-08-24T08:46:48.352883Z","shell.execute_reply":"2023-08-24T08:46:48.387981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 这个模型是专门用来把文本编码成可以比较的相似性向量的模型，可以对维基百科上面的数据进行编码\nmodel = SentenceTransformer(SIM_MODEL, device='cuda')\nmodel.max_seq_length = MAX_LENGTH\nmodel = model.half()","metadata":{"papermill":{"duration":13.282604,"end_time":"2023-08-14T10:09:44.992949","exception":false,"start_time":"2023-08-14T10:09:31.710345","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-24T08:46:48.390446Z","iopub.execute_input":"2023-08-24T08:46:48.390811Z","iopub.status.idle":"2023-08-24T08:46:51.268475Z","shell.execute_reply.started":"2023-08-24T08:46:48.390779Z","shell.execute_reply":"2023-08-24T08:46:51.267496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 因为推理时间有限，所以我们线下直接把维基百科的向量编码好，然后封装进index里面\nsentence_index = read_index(\"/kaggle/input/wikipedia-2023-07-faiss-index/wikipedia_202307.index\")","metadata":{"papermill":{"duration":95.926417,"end_time":"2023-08-14T10:11:20.934445","exception":false,"start_time":"2023-08-14T10:09:45.008028","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-24T08:46:51.269913Z","iopub.execute_input":"2023-08-24T08:46:51.270315Z","iopub.status.idle":"2023-08-24T08:48:21.688492Z","shell.execute_reply.started":"2023-08-24T08:46:51.270277Z","shell.execute_reply":"2023-08-24T08:48:21.687444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 因为推理的时候，隐藏数据的多项选择题是无法离线编码的，所以我们把这个问题用这个模型进行编码，编码成一个向量\nprompt_embeddings = model.encode(trn.prompt.values, batch_size=BATCH_SIZE, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\nprompt_embeddings = prompt_embeddings.detach().cpu().numpy()\n_ = gc.collect()","metadata":{"papermill":{"duration":10.891104,"end_time":"2023-08-14T10:11:31.84869","exception":false,"start_time":"2023-08-14T10:11:20.957586","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-24T08:48:21.692769Z","iopub.execute_input":"2023-08-24T08:48:21.693106Z","iopub.status.idle":"2023-08-24T08:48:30.376755Z","shell.execute_reply.started":"2023-08-24T08:48:21.693078Z","shell.execute_reply":"2023-08-24T08:48:30.375555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 将上面检索的向量在index文件里面进行search，search之后我们得到最相关的索引，根据这个索引我们到对应维基百科的文章里面去找到对应的文本，把它作为context加到隐藏数据里面去\nsearch_score, search_index = sentence_index.search(prompt_embeddings, 5)","metadata":{"papermill":{"duration":23.339585,"end_time":"2023-08-14T10:11:55.247556","exception":false,"start_time":"2023-08-14T10:11:31.907971","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-24T08:48:30.378335Z","iopub.execute_input":"2023-08-24T08:48:30.378737Z","iopub.status.idle":"2023-08-24T08:48:58.508241Z","shell.execute_reply.started":"2023-08-24T08:48:30.378701Z","shell.execute_reply":"2023-08-24T08:48:58.507402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del sentence_index\ndel prompt_embeddings\n_ = gc.collect()\nlibc.malloc_trim(0)","metadata":{"papermill":{"duration":0.877305,"end_time":"2023-08-14T10:11:56.145444","exception":false,"start_time":"2023-08-14T10:11:55.268139","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-24T08:48:58.514846Z","iopub.execute_input":"2023-08-24T08:48:58.515793Z","iopub.status.idle":"2023-08-24T08:48:59.396542Z","shell.execute_reply.started":"2023-08-24T08:48:58.515753Z","shell.execute_reply":"2023-08-24T08:48:59.395397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Getting Sentences from the Relevant Titles","metadata":{}},{"cell_type":"code","source":"df = pd.read_parquet(\"/kaggle/input/wikipedia-20230701/wiki_2023_index.parquet\",\n                     columns=['id', 'file'])","metadata":{"papermill":{"duration":5.737408,"end_time":"2023-08-14T10:12:01.897408","exception":false,"start_time":"2023-08-14T10:11:56.16","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-24T08:48:59.397904Z","iopub.execute_input":"2023-08-24T08:48:59.398357Z","iopub.status.idle":"2023-08-24T08:49:04.654035Z","shell.execute_reply.started":"2023-08-24T08:48:59.398322Z","shell.execute_reply":"2023-08-24T08:49:04.653018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Get the article and associated file location using the index\nwikipedia_file_data = []\n\nfor i, (scr, idx) in tqdm(enumerate(zip(search_score, search_index)), total=len(search_score)):\n    scr_idx = idx\n    _df = df.loc[scr_idx].copy()\n    _df['prompt_id'] = i\n    wikipedia_file_data.append(_df)\nwikipedia_file_data = pd.concat(wikipedia_file_data).reset_index(drop=True)\nwikipedia_file_data = wikipedia_file_data[['id', 'prompt_id', 'file']].drop_duplicates().sort_values(['file', 'id']).reset_index(drop=True)\n\n## Save memory - delete df since it is no longer necessary\ndel df\n_ = gc.collect()\nlibc.malloc_trim(0)","metadata":{"papermill":{"duration":0.799872,"end_time":"2023-08-14T10:12:02.712752","exception":false,"start_time":"2023-08-14T10:12:01.91288","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-24T08:49:04.657394Z","iopub.execute_input":"2023-08-24T08:49:04.657794Z","iopub.status.idle":"2023-08-24T08:49:05.415867Z","shell.execute_reply.started":"2023-08-24T08:49:04.657758Z","shell.execute_reply":"2023-08-24T08:49:05.414885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## 使用索引获取文章和相关的文件位置\nwikipedia_file_data = []  # 初始化一个空列表，用于存储文章和文件位置的数据\n\n# 使用tqdm库显示进度条，遍历search_score和search_index的组合\nfor i, (scr, idx) in tqdm(enumerate(zip(search_score, search_index)), total=len(search_score)):\n    scr_idx = idx  # 获取当前索引\n    _df = df.loc[scr_idx].copy()  # 使用.loc方法从原始数据框中获取与当前索引相关的行，并创建其副本\n    _df['prompt_id'] = i  # 向_df数据框中添加一个新列'prompt_id'，并将其设置为当前迭代的索引i\n    wikipedia_file_data.append(_df)  # 将_df数据框添加到wikipedia_file_data列表中\n\n# 使用pd.concat方法将wikipedia_file_data列表中的所有数据框合并成一个新的数据框\nwikipedia_file_data = pd.concat(wikipedia_file_data).reset_index(drop=True)\n\n# 选择'id', 'prompt_id', 'file'列，并删除重复项，然后按'file'和'id'列对数据框进行排序\nwikipedia_file_data = wikipedia_file_data[['id', 'prompt_id', 'file']].drop_duplicates().sort_values(['file', 'id']).reset_index(drop=True)\n\n## 保存内存 - 删除df，因为它不再需要\ndel df  # 删除原始数据框df，以释放内存\n_ = gc.collect()  # 使用Python的垃圾收集机制，释放不再使用的对象所占用的内存\nlibc.malloc_trim(0)  # 调用C库的malloc_trim函数，以释放C堆上的未使用内存","metadata":{"papermill":{"duration":303.981049,"end_time":"2023-08-14T10:17:06.710072","exception":false,"start_time":"2023-08-14T10:12:02.729023","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-24T08:49:05.417356Z","iopub.execute_input":"2023-08-24T08:49:05.417725Z","iopub.status.idle":"2023-08-24T08:53:47.761878Z","shell.execute_reply.started":"2023-08-24T08:49:05.417693Z","shell.execute_reply":"2023-08-24T08:53:47.760842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Parse documents into sentences\nprocessed_wiki_text_data = process_documents(wiki_text_data.text.values, wiki_text_data.id.values)","metadata":{"papermill":{"duration":4.491281,"end_time":"2023-08-14T10:17:11.220342","exception":false,"start_time":"2023-08-14T10:17:06.729061","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-24T08:53:47.763474Z","iopub.execute_input":"2023-08-24T08:53:47.763859Z","iopub.status.idle":"2023-08-24T08:53:52.060443Z","shell.execute_reply.started":"2023-08-24T08:53:47.763825Z","shell.execute_reply":"2023-08-24T08:53:52.059359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Get embeddings of the wiki text data\nwiki_data_embeddings = model.encode(processed_wiki_text_data.text,\n                                    batch_size=BATCH_SIZE,\n                                    device=DEVICE,\n                                    show_progress_bar=True,\n                                    convert_to_tensor=True,\n                                    normalize_embeddings=True)#.half()\nwiki_data_embeddings = wiki_data_embeddings.detach().cpu().numpy()","metadata":{"papermill":{"duration":25.110593,"end_time":"2023-08-14T10:17:36.348422","exception":false,"start_time":"2023-08-14T10:17:11.237829","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-24T08:53:52.061804Z","iopub.execute_input":"2023-08-24T08:53:52.062279Z","iopub.status.idle":"2023-08-24T08:54:17.007622Z","shell.execute_reply.started":"2023-08-24T08:53:52.062243Z","shell.execute_reply":"2023-08-24T08:54:17.00661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ = gc.collect()","metadata":{"papermill":{"duration":0.315807,"end_time":"2023-08-14T10:17:36.679867","exception":false,"start_time":"2023-08-14T10:17:36.36406","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-24T08:54:17.009098Z","iopub.execute_input":"2023-08-24T08:54:17.009569Z","iopub.status.idle":"2023-08-24T08:54:17.320019Z","shell.execute_reply.started":"2023-08-24T08:54:17.009534Z","shell.execute_reply":"2023-08-24T08:54:17.318485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Combine all answers\ntrn['answer_all'] = trn.apply(lambda x: \" \".join([x['A'], x['B'], x['C'], x['D'], x['E']]), axis=1)\n\n\n## Search using the prompt and answers to guide the search\ntrn['prompt_answer_stem'] = trn['prompt'] + \" \" + trn['answer_all']","metadata":{"papermill":{"duration":0.034767,"end_time":"2023-08-14T10:17:36.730378","exception":false,"start_time":"2023-08-14T10:17:36.695611","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-24T08:54:17.321961Z","iopub.execute_input":"2023-08-24T08:54:17.322503Z","iopub.status.idle":"2023-08-24T08:54:17.341921Z","shell.execute_reply.started":"2023-08-24T08:54:17.322467Z","shell.execute_reply":"2023-08-24T08:54:17.340954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"question_embeddings = model.encode(trn.prompt_answer_stem.values, batch_size=BATCH_SIZE, device=DEVICE, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\nquestion_embeddings = question_embeddings.detach().cpu().numpy()","metadata":{"papermill":{"duration":0.431343,"end_time":"2023-08-14T10:17:37.177862","exception":false,"start_time":"2023-08-14T10:17:36.746519","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-24T08:54:17.343506Z","iopub.execute_input":"2023-08-24T08:54:17.343908Z","iopub.status.idle":"2023-08-24T08:54:17.692524Z","shell.execute_reply.started":"2023-08-24T08:54:17.34387Z","shell.execute_reply":"2023-08-24T08:54:17.691481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Extracting Matching Prompt-Sentence Pairs","metadata":{}},{"cell_type":"code","source":"## 参数，确定要包含多少相关句子\nNUM_SENTENCES_INCLUDE = 20\n\n## 只包含上下文的列表\ncontexts = []\n\n# 使用tqdm库显示进度条，遍历trn数据框的每一行\nfor r in tqdm(trn.itertuples(), total=len(trn)):\n\n    prompt_id = r.Index  # 获取当前行的索引\n\n    # 查找与当前prompt_id相关的文档ID，并获取它们在processed_wiki_text_data数据框中的索引\n    prompt_indices = processed_wiki_text_data[processed_wiki_text_data['document_id'].isin(wikipedia_file_data[wikipedia_file_data['prompt_id']==prompt_id]['id'].values)].index.values\n\n    # 检查是否找到了与当前prompt_id相关的文档ID\n    if prompt_indices.shape[0] > 0:\n        # 创建一个FAISS索引，用于存储与当前prompt_id相关的文档的嵌入\n        prompt_index = faiss.index_factory(wiki_data_embeddings.shape[1], \"Flat\")\n        # 将与当前prompt_id相关的文档的嵌入添加到FAISS索引中\n        prompt_index.add(wiki_data_embeddings[prompt_indices])\n\n        context = \"\"  # 初始化一个空字符串，用于存储上下文\n        \n        ## 获取最佳匹配\n        # 使用FAISS索引搜索与question_embeddings最相似的NUM_SENTENCES_INCLUDE个句子\n        ss, ii = prompt_index.search(question_embeddings, NUM_SENTENCES_INCLUDE)\n        # 遍历最佳匹配的得分和索引\n        for _s, _i in zip(ss[prompt_id], ii[prompt_id]):\n            # 从processed_wiki_text_data数据框中获取匹配句子的文本，并将其添加到上下文中\n            context += processed_wiki_text_data.loc[prompt_indices]['text'].iloc[_i] + \" \"\n        \n    contexts.append(context)  # 将上下文添加到contexts列表中\n","metadata":{"papermill":{"duration":1.609553,"end_time":"2023-08-14T10:17:38.836268","exception":false,"start_time":"2023-08-14T10:17:37.226715","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-24T08:54:17.69404Z","iopub.execute_input":"2023-08-24T08:54:17.694652Z","iopub.status.idle":"2023-08-24T08:54:19.115363Z","shell.execute_reply.started":"2023-08-24T08:54:17.694617Z","shell.execute_reply":"2023-08-24T08:54:19.114281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trn['context'] = contexts","metadata":{"papermill":{"duration":0.024188,"end_time":"2023-08-14T10:17:38.878394","exception":false,"start_time":"2023-08-14T10:17:38.854206","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-24T08:54:19.11873Z","iopub.execute_input":"2023-08-24T08:54:19.119018Z","iopub.status.idle":"2023-08-24T08:54:19.127232Z","shell.execute_reply.started":"2023-08-24T08:54:19.118992Z","shell.execute_reply":"2023-08-24T08:54:19.12624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 这样隐藏数据和训练数据都对应起来了\ntrn[[\"prompt\", \"context\", \"A\", \"B\", \"C\", \"D\", \"E\"]].to_csv(\"./test_context.csv\", index=False)","metadata":{"papermill":{"duration":0.050945,"end_time":"2023-08-14T10:17:38.944423","exception":false,"start_time":"2023-08-14T10:17:38.893478","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-24T08:54:19.128686Z","iopub.execute_input":"2023-08-24T08:54:19.129114Z","iopub.status.idle":"2023-08-24T08:54:19.159307Z","shell.execute_reply.started":"2023-08-24T08:54:19.129078Z","shell.execute_reply":"2023-08-24T08:54:19.158343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{"papermill":{"duration":0.015828,"end_time":"2023-08-14T10:17:39.007683","exception":false,"start_time":"2023-08-14T10:17:38.991855","status":"completed"},"tags":[]}},{"cell_type":"code","source":"test_df = pd.read_csv(\"test_context.csv\")\ntest_df.index = list(range(len(test_df)))\ntest_df['id'] = list(range(len(test_df)))\ntest_df[\"prompt\"] = test_df[\"context\"].apply(lambda x: x[:1750]) + \" #### \" +  test_df[\"prompt\"]\ntest_df['answer'] = 'A'","metadata":{"papermill":{"duration":0.037633,"end_time":"2023-08-14T10:17:39.605345","exception":false,"start_time":"2023-08-14T10:17:39.567712","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-24T08:54:19.162655Z","iopub.execute_input":"2023-08-24T08:54:19.162918Z","iopub.status.idle":"2023-08-24T08:54:19.180183Z","shell.execute_reply.started":"2023-08-24T08:54:19.162895Z","shell.execute_reply":"2023-08-24T08:54:19.179331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_dir = \"/kaggle/input/llm-science-run-context-2\"\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\nmodel = AutoModelForMultipleChoice.from_pretrained(model_dir).cuda()\nmodel.eval()","metadata":{"papermill":{"duration":21.360878,"end_time":"2023-08-14T10:18:01.027859","exception":false,"start_time":"2023-08-14T10:17:39.666981","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-24T08:54:19.182911Z","iopub.execute_input":"2023-08-24T08:54:19.183733Z","iopub.status.idle":"2023-08-24T08:54:39.509176Z","shell.execute_reply.started":"2023-08-24T08:54:19.183699Z","shell.execute_reply":"2023-08-24T08:54:39.508018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 我们将创建一个字典，以将选项名称（A、B、C、D、E）转换为索引，并反过来再转换回选项名称\noptions = 'ABCDE'  # 选项名称\nindices = list(range(5))  # 对应的索引值，即0到4\n\n# 创建选项到索引的字典\noption_to_index = {option: index for option, index in zip(options, indices)}\n# 创建索引到选项的字典\nindex_to_option = {index: option for option, index in zip(options, indices)}\n\n# 定义预处理函数\ndef preprocess(example):\n    # AutoModelForMultipleChoice类期望得到一组问题/答案对，\n    # 所以我们将问题复制5次然后再进行标记化（tokenization）\n    first_sentence = [example['prompt']] * 5  # 复制问题5次\n    second_sentence = []  # 初始化一个空列表来存储选项文本\n    for option in options:\n        second_sentence.append(example[option])  # 将每个选项的文本添加到second_sentence列表中\n    \n    # 我们的标记器（tokenizer）会将文本转换为BERT可以理解的标记ID\n    # 这里假设tokenizer已经被正确定义和初始化\n    tokenized_example = tokenizer(first_sentence, second_sentence, truncation=True)\n    \n    # 将正确答案的选项名称转换为索引，并添加到tokenized_example字典中\n    tokenized_example['label'] = option_to_index[example['answer']]\n    \n    return tokenized_example  # 返回标记化的样本","metadata":{"papermill":{"duration":0.026162,"end_time":"2023-08-14T10:18:01.129276","exception":false,"start_time":"2023-08-14T10:18:01.103114","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-24T08:54:39.511057Z","iopub.execute_input":"2023-08-24T08:54:39.511439Z","iopub.status.idle":"2023-08-24T08:54:39.519617Z","shell.execute_reply.started":"2023-08-24T08:54:39.511405Z","shell.execute_reply":"2023-08-24T08:54:39.518608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@dataclass  # 使用dataclass装饰器简化类的定义\nclass DataCollatorForMultipleChoice:\n    tokenizer: PreTrainedTokenizerBase  # 预训练的标记器\n    padding: Union[bool, str, PaddingStrategy] = True  # 填充策略，可以是布尔值、字符串或PaddingStrategy枚举值\n    max_length: Optional[int] = None  # 可选的最大长度参数，如果提供，则所有序列都将被截断或填充到此长度\n    pad_to_multiple_of: Optional[int] = None  # 如果提供，所有序列的长度将填充到此值的倍数\n    \n    def __call__(self, features):  # 定义__call__方法使得这个类的实例可以像函数一样被调用\n        # 确定标签的键名是\"label\"还是\"labels\"\n        label_name = \"label\" if 'label' in features[0].keys() else 'labels'\n        # 从特征中提取并删除标签\n        labels = [feature.pop(label_name) for feature in features]\n        batch_size = len(features)  # 批次大小\n        num_choices = len(features[0]['input_ids'])  # 选项数量\n        # 将特征展平以适应标记器的pad方法\n        flattened_features = [\n            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n        ]\n        flattened_features = sum(flattened_features, [])  # 将特征列表展平\n        \n        # 使用标记器的pad方法进行填充，并返回张量\n        batch = self.tokenizer.pad(\n            flattened_features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors='pt',  # 返回PyTorch张量\n        )\n        # 调整张量的维度以匹配期望的批处理结构\n        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n        # 将标签转换为张量并添加到批处理中\n        batch['labels'] = torch.tensor(labels, dtype=torch.int64)\n        return batch  # 返回批处理","metadata":{"papermill":{"duration":0.030447,"end_time":"2023-08-14T10:18:01.175589","exception":false,"start_time":"2023-08-14T10:18:01.145142","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-24T08:54:39.521323Z","iopub.execute_input":"2023-08-24T08:54:39.521995Z","iopub.status.idle":"2023-08-24T08:54:39.543196Z","shell.execute_reply.started":"2023-08-24T08:54:39.521961Z","shell.execute_reply":"2023-08-24T08:54:39.542203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_test_dataset = Dataset.from_pandas(test_df[['id', 'prompt', 'A', 'B', 'C', 'D', 'E', 'answer']].drop(columns=['id'])).map(preprocess, remove_columns=['prompt', 'A', 'B', 'C', 'D', 'E', 'answer'])\ntokenized_test_dataset = tokenized_test_dataset.remove_columns([\"__index_level_0__\"])\ndata_collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\ntest_dataloader = DataLoader(tokenized_test_dataset, batch_size=1, shuffle=False, collate_fn=data_collator)","metadata":{"papermill":{"duration":0.493618,"end_time":"2023-08-14T10:18:01.685989","exception":false,"start_time":"2023-08-14T10:18:01.192371","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-24T08:54:39.54468Z","iopub.execute_input":"2023-08-24T08:54:39.545012Z","iopub.status.idle":"2023-08-24T08:54:40.542225Z","shell.execute_reply.started":"2023-08-24T08:54:39.544981Z","shell.execute_reply":"2023-08-24T08:54:40.541204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_predictions = []\nfor batch in test_dataloader:\n    for k in batch.keys():\n        batch[k] = batch[k].cuda()\n    with torch.no_grad():\n        outputs = model(**batch)\n    test_predictions.append(outputs.logits.cpu().detach())\n\ntest_predictions = torch.cat(test_predictions)\ntest_predictions = test_predictions.numpy()","metadata":{"papermill":{"duration":1.101895,"end_time":"2023-08-14T10:18:02.804298","exception":false,"start_time":"2023-08-14T10:18:01.702403","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-24T08:54:40.543987Z","iopub.execute_input":"2023-08-24T08:54:40.544372Z","iopub.status.idle":"2023-08-24T08:55:26.501433Z","shell.execute_reply.started":"2023-08-24T08:54:40.544338Z","shell.execute_reply":"2023-08-24T08:55:26.500414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Model From Our Train Notebook","metadata":{}},{"cell_type":"code","source":"model_dir = \"/kaggle/input/how-to-train-open-book-model/model_v2\"\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\nmodel = AutoModelForMultipleChoice.from_pretrained(model_dir).cuda()\nmodel.eval()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_predictions2 = []\nfor batch in test_dataloader:\n    for k in batch.keys():\n        batch[k] = batch[k].cuda()\n    with torch.no_grad():\n        outputs = model(**batch)\n    test_predictions2.append(outputs.logits.cpu().detach())\n\ntest_predictions2 = torch.cat(test_predictions2)\ntest_predictions = (test_predictions+test_predictions2.numpy()) / 2.0\n\npredictions_as_ids = np.argsort(-test_predictions, 1)\n\npredictions_as_answer_letters = np.array(list('ABCDE'))[predictions_as_ids]\n# predictions_as_answer_letters[:3]\n\npredictions_as_string = test_df['prediction'] = [\n    ' '.join(row) for row in predictions_as_answer_letters[:, :3]\n]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = test_df[['id', 'prediction']]\nsubmission.to_csv('submission.csv', index=False)","metadata":{"papermill":{"duration":0.033576,"end_time":"2023-08-14T10:19:17.733491","exception":false,"start_time":"2023-08-14T10:19:17.699915","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-08-24T08:55:26.506831Z","iopub.execute_input":"2023-08-24T08:55:26.507231Z","iopub.status.idle":"2023-08-24T08:55:26.515127Z","shell.execute_reply.started":"2023-08-24T08:55:26.507183Z","shell.execute_reply":"2023-08-24T08:55:26.514093Z"},"trusted":true},"execution_count":null,"outputs":[]}]}