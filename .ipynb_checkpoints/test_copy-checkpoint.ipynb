{"cells":[{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["import numpy as np \n","import matplotlib.pyplot as plt \n","from scipy.linalg import expm \n","from datetime import datetime\n","import math"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# generate data \n","x = np.asarray([[1,1,-1,-1],[1,-1,1,-1]])\n","y = np.array([1,-1,-1,1])\n","# plt.figure()\n","# plt.scatter(x[0,:],x[1,:],c=y)\n","# plt.title('XOR Problem')\n","# y = y[np.newaxis,:]"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["from numpy.linalg import norm\n","# define a couple of helper functions\n","def relu(x):\n","     return x*(x>0)\n","def deriv_relu(x):\n","    return (x>0).astype('double')\n","\n","def loss(y_,y):\n","    return .5*norm(y_-y,2)**2\n","def deriv_loss(y_,y): \n","    return (y_-y)\n","\n","def compute_relchange(w0,wt):    \n","    return (norm(wt)-norm(w0))/norm(w0)"]},{"cell_type":"markdown","metadata":{},"source":["### 1. Compute the H\n","to calculate H by collecting the partial derivatives and setting up the gradient of the network function. then it needs to stack the individual gradients - for input and readout weights - on top of each other.\n","As a reminder, these were the partial derivatives wrt the input and readout weights: \n","\n","$$\n","\\Delta w_{yh} = \\nabla L_{w_{yh}} = \\frac{\\partial L}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial w_{yh}}\n","$$\n","$$\n","\\Delta w_{yh} = (\\hat{y}-y)h_{out}\n","$$\n","$$ \n","\\Delta w_{hx} = \\nabla L_{w_{hx}} = \\frac{\\partial L}{\\partial \\hat{y}}  \\frac{\\partial \\hat{y}}{\\partial h_{out}} \\odot \\frac{\\partial h_{out}}{\\partial h_{in}}  \\frac{\\partial h_{in}}{\\partial w_{hx}}\n","$$\n","$$\n","\\Delta w_{hx} =  (\\hat{y}-y)w_{yh}\\odot (\\mathbf{1}_{(h_{in}>0)} h_{in}) x\n","$$ "]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(4, 4)\n"]}],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","\n","# Input data\n","x = np.asarray([[1, 0.5, 0.5, 1], [1, 0.0025, 0.025, 1]])\n","y = np.array([1, .25, .25, 1])\n","\n","# Define parameters\n","n_in = 2 # number of layers\n","n_hidden = 32  # Adjusted to make  \n","n_out = 1 # output layer\n","scale_w1 = 0.5  # 1/np.sqrt(2)\n","scale_w2 = 1 / n_hidden\n","\n","# Initialize weights\n","w_hx = scale_w1 * np.abs(np.random.randn(n_hidden, n_in))\n","w_yh = scale_w2 * np.abs(np.random.randn(n_out, n_hidden))\n","\n","# Store initial weights\n","w_hx_0 = w_hx\n","w_yh_0 = w_yh\n","\n","# Cycle through datapoints and collect gradients\n","all_gradients = np.empty((n_hidden * (n_in + n_out), x.shape[1]))\n","for ii in range(x.shape[1]):\n","    xi = x[:, ii].reshape(2, 1)\n","    yi = y[ii].reshape(1, 1)\n","\n","    # Forward pass\n","    h_in = w_hx.dot(xi)\n","    h_out = relu(h_in)\n","    y_ = w_yh.dot(h_out)\n","    l = loss(yi, y_)\n","    # Partial derivatives\n","    dl_dy = deriv_loss(y_, yi)\n","    dy_dh = w_yh\n","    dy_dw = h_out\n","    dho_dhi = deriv_relu(h_in)\n","    dhi_dw = xi\n","    # Chain rule\n","    dl_dwyh = dl_dy.dot(dy_dw.T)\n","    dl_dwhx = (dl_dy.T.dot(dy_dh) * dho_dhi.T).T.dot(dhi_dw.T)\n","    # Add to collection\n","    grad_vect = np.concatenate((dl_dwyh.flatten(), dl_dwhx.flatten()))\n","    all_gradients[:, ii] = grad_vect\n","\n","# Compute the matrix (dot product of gradients)\n","H = np.dot(all_gradients.T, all_gradients)\n","print(H.shape)\n","\n","# print(x.shape)\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(32, 32)\n"]}],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Define a few parameters\n","n_in = 5  # Input size\n","n_hidden = 100  # Hidden layer size\n","n_out = 1  # Output size\n","scale_w1 = 0.5\n","scale_w2 = 1 / n_hidden\n","\n","# Initialize weights\n","w_hx = scale_w1 * (np.random.randn(n_hidden, n_in))\n","w_yh = scale_w2 * (np.random.randn(n_out, n_hidden))\n","\n","# Store initial weights\n","w_hx_0 = w_hx\n","w_yh_0 = w_yh\n","\n","# Generate random input data (you need to replace this with your actual input data)\n","x = np.random.randn(n_in, 32)\n","# Generate random output data (you need to replace this with your actual output data)\n","y = np.random.randn(1, 32)\n","\n","# Define activation function (ReL\n","\n","\n","\n","# Cycle through datapoints and collect gradients\n","all_gradients = np.empty((n_hidden * (n_in + n_out), x.shape[1]))\n","for ii in range(x.shape[1]):\n","    xi = x[:, ii].reshape(n_in, 1)\n","    yi = y[0][ii].reshape(1, 1)\n","\n","    # Forward pass\n","    h_in = w_hx.dot(xi)\n","    h_out = relu(h_in)\n","    y_ = w_yh.dot(h_out)\n","    # l = loss(yi, y_)\n","    # Partial derivatives\n","    dl_dy = deriv_loss(y_, yi)\n","    dy_dh = w_yh\n","    dy_dw = h_out\n","    dho_dhi = deriv_relu(h_in)\n","    dhi_dw = xi\n","    # Chain rule\n","    dl_dwyh = dl_dy.dot(dy_dw.T)\n","    dl_dwhx = (dl_dy.T.dot(dy_dh) * dho_dhi.T).T.dot(dhi_dw.T)\n","    # Add to collection\n","    grad_vect = np.concatenate((dl_dwyh.flatten(), dl_dwhx.flatten()))\n","    all_gradients[:, ii] = grad_vect\n","\n","# Compute the matrix (dot product of gradients)\n","H = np.dot(all_gradients.T, all_gradients)\n","print(H.shape)\n","\n","\n"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The matrix is symmetric.\n"]}],"source":["####Symetric###\n","\n","is_symmetric = np.allclose(H, H.T)\n","\n","if is_symmetric:\n","    print(\"The matrix is symmetric.\")\n","else:\n","    print(\"The matrix is not symmetric.\")"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"data":{"text/plain":["True"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["def is_pos_def(x):\n","    return np.all(np.linalg.eigvals(x) > 0)\n","\n","is_pos_def(H)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/plain":["array([[3.35798065e-02, 4.04274663e-03, 1.25151005e-03, 3.35798065e-02],\n","       [4.04274663e-03, 6.11150720e-04, 1.85777088e-04, 4.04274663e-03],\n","       [1.25151005e-03, 1.85777088e-04, 5.65470641e-05, 1.25151005e-03],\n","       [3.35798065e-02, 4.04274663e-03, 1.25151005e-03, 3.35798065e-02]])"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["H "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"file_extension":".py","kernelspec":{"display_name":"nt_pooling","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"},"mimetype":"text/x-python","name":"python","npconvert_exporter":"python","orig_nbformat":2,"pygments_lexer":"ipython3","version":3},"nbformat":4,"nbformat_minor":2}
